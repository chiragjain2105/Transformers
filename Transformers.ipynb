{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jM8hfIKBrwB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "  d_k = q.size()[-1]\n",
        "  scaled = torch.matmul(q,k.transpose(-1,-2))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled += mask\n",
        "  attention = F.softmax(scaled,dim=-1)\n",
        "  values = torch.matmul(attention,v)\n",
        "  return values,attention"
      ],
      "metadata": {
        "id": "783YBazuFOT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
        "    self.linear_layer = nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size, sequence_length, d_model = x.size()\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3*self.head_dim)\n",
        "    qkv = qkv.permute(0,2,1,3)\n",
        "    q,k,v = qkv.chunk(3,dim=-1)\n",
        "    values,attention = scaled_dot_product(q,k,v,mask)\n",
        "    values = values.reshape(batch_size,sequence_length,self.num_heads*self.head_dim)\n",
        "    out = self.linear_layer(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "kJxPB_0mFOW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "    self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = inputs.mean(dim=dims,keepdim=True)\n",
        "    var = ((inputs-mean)**2).mean(dim=dims,keepdim=True)\n",
        "    std = (var+self.eps).sqrt()\n",
        "    y = (inputs-mean)/std\n",
        "    out = self.gamma*y + self.beta\n",
        "    return out"
      ],
      "metadata": {
        "id": "fF-byWYvNsH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,hidden,drop_prob=0.1):\n",
        "    super(PositionwiseFeedForward,self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model,hidden)\n",
        "    self.linear2 = nn.Linear(hidden,d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vSXSj6GFNsPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual_x = x\n",
        "    x = self.attention(x,mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x+residual_x)\n",
        "    residual_x = x\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x+residual_x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ex8yZkxdFOab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  # encoder inherit 'Module' class from pytorch\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.layers(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PjbmiP1CBvqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the parameters\n",
        "d_model = 512 # size of every single vector throughout he encoder\n",
        "num_heads = 8 # no. of sets of k,q,v vectors multi-head attention\n",
        "drop_prob = 0.1 # dropout for reqularization\n",
        "batch_size = 30 # passing 30 examples same time (Batch Gradient Descent)\n",
        "max_sequence_length = 200 # maximum length of sentence\n",
        "ffn_hidden = 2048 # number of neurons in ffnn\n",
        "num_layers = 5 # number of encoders and decoders in stack\n",
        "\n",
        "encoder = Encoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers)"
      ],
      "metadata": {
        "id": "-dRCrLmwBvtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfx7dVkyTuTI",
        "outputId": "90cac599-c847-41fb-f8c1-72b659f9f65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (layers): Sequential(\n",
              "    (0): EncoderLayer(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (ffn): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm2): LayerNormalization()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (ffn): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm2): LayerNormalization()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (ffn): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm2): LayerNormalization()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (ffn): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm2): LayerNormalization()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNormalization()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (ffn): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm2): LayerNormalization()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.kv_layer = nn.Linear(d_model,2*d_model)\n",
        "    self.q_layer = nn.Linear(d_model,d_model)\n",
        "    self.linear_layer = nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,y,mask=None):\n",
        "    batch_size, sequence_length, d_model = x.size()\n",
        "    kv = self.kv_layer(x)\n",
        "    q = self.q_layer(y)\n",
        "    kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2*self.head_dim)\n",
        "    q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "    kv = kv.permute(0,2,1,3)\n",
        "    q = q.permute(0,2,1,3)\n",
        "    k,v = kv.chunk(2,dim=-1)\n",
        "    values,attention = scaled_dot_product(q,k,v,mask)\n",
        "    values = values.reshape(batch_size,sequence_length,d_model)\n",
        "    out = self.linear_layer(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "_X7DZRPkwvd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.self_attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, x, y, decoder_mask):\n",
        "      _y = y # 30 x 200 x 512\n",
        "      print(\"MASKED SELF ATTENTION\")\n",
        "      y = self.self_attention(y, mask=decoder_mask) # 30 x 200 x 512\n",
        "      print(\"DROP OUT 1\")\n",
        "      y = self.dropout1(y) # 30 x 200 x 512\n",
        "      print(\"ADD + LAYER NORMALIZATION 1\")\n",
        "      y = self.norm1(y + _y) # 30 x 200 x 512\n",
        "\n",
        "      _y = y # 30 x 200 x 512\n",
        "      print(\"CROSS ATTENTION\")\n",
        "      y = self.encoder_decoder_attention(x, y, mask=None) #30 x 200 x 512\n",
        "      print(\"DROP OUT 2\")  #30 x 200 x 512\n",
        "      y = self.dropout2(y)\n",
        "      print(\"ADD + LAYER NORMALIZATION 2\")\n",
        "      y = self.norm2(y + _y)  #30 x 200 x 512\n",
        "\n",
        "      _y = y  #30 x 200 x 512\n",
        "      print(\"FEED FORWARD 1\")\n",
        "      y = self.ffn(y) #30 x 200 x 512\n",
        "      print(\"DROP OUT 3\")\n",
        "      y = self.dropout3(y) #30 x 200 x 512\n",
        "      print(\"ADD + LAYER NORMALIZATION 3\")\n",
        "      y = self.norm3(y + _y) #30 x 200 x 512\n",
        "      return y #30 x 200 x 512\n",
        "\n"
      ],
      "metadata": {
        "id": "-wTCeHs5wvfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "  def forward(self,*inputs):\n",
        "    x,y,mask = inputs\n",
        "    for module in self._modules.values():\n",
        "      y = module(x,y,mask)\n",
        "    return y"
      ],
      "metadata": {
        "id": "Vx-wJvpPwvit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers=1):\n",
        "    super().__init__()\n",
        "    self.layers = SequentialDecoder(*[DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
        "  def forward(self,x,y,mask):\n",
        "    # x:30*200*512\n",
        "    # y:30*200*512\n",
        "    # mask:200*200\n",
        "    y = self.layers(x,y,mask)\n",
        "    return y"
      ],
      "metadata": {
        "id": "dGNh7oXcwvl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the parameters\n",
        "d_model = 512 # size of every single vector throughout he encoder\n",
        "num_heads = 8 # no. of sets of k,q,v vectors multi-head attention\n",
        "drop_prob = 0.1 # dropout for reqularization\n",
        "batch_size = 30 # passing 30 examples same time (Batch Gradient Descent)\n",
        "max_sequence_length = 200 # maximum length of sentence\n",
        "ffn_hidden = 2048 # number of neurons in ffnn\n",
        "num_layers = 5 # number of encoders and decoders in stack"
      ],
      "metadata": {
        "id": "EJq1DG4o2_tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((batch_size,max_sequence_length,d_model)) # encoded english sentence\n",
        "y = torch.randn((batch_size,max_sequence_length,d_model)) # encoded hindi sentence\n",
        "mask = torch.full([max_sequence_length,max_sequence_length], float('-inf'))\n",
        "mask = torch.triu(mask,diagonal=1)\n",
        "decoder = Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers)\n",
        "out = decoder(x,y,mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL3uvZc6T3kA",
        "outputId": "1c678640-d09e-49e5-9128-d6b8f69cd575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASKED SELF ATTENTION\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIkpNVaQ3BEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}